GENERAL:
  GPU_ID: 3
  SEED: 123
  WORKER: 8
  SAVE_PREFIX: './out_dir/fhdmi'
  EXP_NAME: 'exp_large'

DATA:
  DATA_TYPE: FHDMi # Please specify the type of the dataset (select from AIM/UHDM/FHDMi/TIP)
  TRAIN_DATASET: # The training data path, e.g., ./uhdm_data/Train
  TEST_DATASET: # The test data path, e.g., ./uhdm_data/Test

MODEL:
  EN_FEATURE_NUM: 48 # The initial channel number of dense blocks of encoder
  EN_INTER_NUM: 32 # The growth rate (intermediate channel number) of dense blocks of encoder
  DE_FEATURE_NUM: 64 # The initial channel number of dense blocks of decoder
  DE_INTER_NUM: 32 # The growth rate (intermediate channel number) of dense blocks of decoder
  SAM_NUMBER: 2 # The number of SAM for each encoder or decoder level; set 1 for our ESDNet, and 2 for ESDNet-L

TRAIN:
  BATCH_SIZE: 2
  LOADER: crop # The loading way for training data, e.g., crop, resize, default; see ./dataset/load_data.py
  CROP_SIZE: 512 # Set the crop size if LOADER==crop
  RESIZE_SIZE: 384 # Set the resizing size if LOADER==crop
  SAVE_ITER: 500 # Save training images/results at each SAVE_ITER*n iter
  LOAD_EPOCH: False # If specify it, loading the corresponding model for resuming training
  LAM: 1 # The loss weight for L1 loss
  LAM_P: 1 # The loss weight for perceptual loss

TEST:
  TEST_EPOCH: 150 # Input 'auto' for loading the latest model
  SAVE_IMG: False # The file type (e.g., jpg, png) for saving the output image; set False to avoid saving
  LOAD_PATH: False # If specify a load path for a checkpoint, TEST_EPOCH will be deprecated
  EVALUATION_METRIC: True # If True, calculate metrics
  EVALUATION_TIME: False # If True, calculate processing time per image; EVALUATION_METRIC will be deprecated for accurate statistics
  EVALUATION_COST: False #If True, calculate MACs and Parameters number

SOLVER:
  EPOCHS: 150 # The total training epochs
  T_0: 50 # The total epochs for the first learning cycle (learning rate warms up then)
  T_MULT: 1 # The learning cycle would be (T_0, T_0*T_MULT, T_0*T_MULT^2, T_0*T_MULT^3, ...)
  ETA_MIN: 0.000001 # Initial learning rate in each learning cycle
  BASE_LR: 0.0002 # Learning rate in the end of each learning cycle

